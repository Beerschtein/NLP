{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aed6788",
   "metadata": {},
   "source": [
    "## ДЗ_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdb02a3",
   "metadata": {},
   "source": [
    "### Задание 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6530e9a2",
   "metadata": {},
   "source": [
    "Обучить три классификатора:\n",
    "1. На токенах с высокой частотой.\n",
    "2. На токенах со средней частотой.\n",
    "3. На токенах с низкой частотой.\n",
    "Сравните полученные результаты, оцените какие токены наиболее важные для классификации."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07591f0f",
   "metadata": {},
   "source": [
    "### Задание 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77262c1",
   "metadata": {},
   "source": [
    "Найти фичи с наибольшей значимостью, и вывести их."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ad598f",
   "metadata": {},
   "source": [
    "### Задание 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9400ec18",
   "metadata": {},
   "source": [
    "1. Сравнить count/tf-idf/hashing векторайзеры/полносвязанную сетку (построить classification_report).\n",
    "2. Подобрать оптимальный размер для hashing векторайзера.\n",
    "3. Убедиться что для сетки нет переобучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb5742af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59ea551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv\n",
    "# !wget https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83868bc3",
   "metadata": {},
   "source": [
    "Считаем данные и создадим объединенный датасет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0aa239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = pd.read_csv('./positive.csv', sep=';', usecols=[3], names=['text'])\n",
    "positive['label'] = ['positive'] * len(positive)\n",
    "negative = pd.read_csv('./negative.csv', sep=';', usecols=[3], names=['text'])\n",
    "negative['label'] = ['negative'] * len(negative)\n",
    "df = positive.append(negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffd3dbd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@first_timee хоть я и школота, но поверь, у на...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Да, все-таки он немного похож на него. Но мой ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @KatiaCheh: Ну ты идиотка) я испугалась за ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @digger2912: \"Кто то в углу сидит и погибае...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@irina_dyshkant Вот что значит страшилка :D\\nН...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     label\n",
       "0  @first_timee хоть я и школота, но поверь, у на...  positive\n",
       "1  Да, все-таки он немного похож на него. Но мой ...  positive\n",
       "2  RT @KatiaCheh: Ну ты идиотка) я испугалась за ...  positive\n",
       "3  RT @digger2912: \"Кто то в углу сидит и погибае...  positive\n",
       "4  @irina_dyshkant Вот что значит страшилка :D\\nН...  positive"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2446da",
   "metadata": {},
   "source": [
    "Разделим данные на train и test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0277eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df.text, df.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2ad7c42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10a99ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "from nltk import collocations \n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b30b47a",
   "metadata": {},
   "source": [
    "*Отсортируем в тексте \"стоп-слова\" и пунктуацию.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4391d4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package genesis to /Users/mac/nltk_data...\n",
      "[nltk_data]   Package genesis is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/mac/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/mac/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('genesis')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "print(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df3a230b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "noise = stopwords.words('russian') + list(punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2ec3a1",
   "metadata": {},
   "source": [
    "*Создадим корпус используемых слов.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3d0937b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2870536\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['first_timee', 'хоть', 'я', 'и', 'школота', 'но', 'поверь', 'у', 'нас', 'то']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [token for tweet in df.text for token in word_tokenize(tweet) if token not in punctuation]\n",
    "print(len(corpus))\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9015604c",
   "metadata": {},
   "source": [
    "*Отсортируем их по частоте употребления и выведем 20 наиболее часто встречающихся.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ca7ca09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('не', 69472),\n",
       " ('и', 55166),\n",
       " ('в', 52902),\n",
       " ('я', 52818),\n",
       " ('RT', 38070),\n",
       " ('на', 35759),\n",
       " ('http', 32998),\n",
       " ('что', 31541),\n",
       " ('с', 27217),\n",
       " ('а', 26860),\n",
       " ('...', 22363),\n",
       " ('меня', 20656),\n",
       " ('у', 18928),\n",
       " ('как', 18279),\n",
       " ('так', 16839),\n",
       " ('D', 16575),\n",
       " ('это', 16542),\n",
       " ('мне', 16337),\n",
       " ('все', 14763),\n",
       " ('ты', 13412)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_dict = Counter(corpus)\n",
    "freq_dict_sorted= sorted(freq_dict.items(), key=lambda x: -x[1])\n",
    "list(freq_dict_sorted)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef95421",
   "metadata": {},
   "source": [
    "### Задание 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1d50aa",
   "metadata": {},
   "source": [
    "*Сначала создадим токены с высокой, средней и низкой частотой.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4721bd12",
   "metadata": {},
   "source": [
    "*В качестве высокочастотных определим токены, которые встречаются более 2000 раз, среднечастотные - встречаются от 500 раз до 2000 раз, остальные будем считать низкочастотными.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20649ae",
   "metadata": {},
   "source": [
    "*Разделение на частоты является условным и носит экспериментальный характер.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0918f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "high-tokens - 132\n",
      "medium-tokens - 404\n",
      "low-tokens - 350587\n"
     ]
    }
   ],
   "source": [
    "high_tokens = set()\n",
    "medium_tokens = set()\n",
    "low_tokens = set()\n",
    "\n",
    "for i in freq_dict_sorted:\n",
    "    if i[1] > 2000:\n",
    "        high_tokens.add(i[0])\n",
    "    elif i[1] < 500:\n",
    "        low_tokens.add(i[0])\n",
    "    else:\n",
    "        medium_tokens.add(i[0])\n",
    "\n",
    "print(f'high-tokens - {len(high_tokens)}')\n",
    "print(f'medium-tokens - {len(medium_tokens)}')\n",
    "print(f'low-tokens - {len(low_tokens)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd32ad16",
   "metadata": {},
   "source": [
    "*Возьмем стандартную модель, обучим ее, выполним предсказание на наших токенах и выведем результат.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e561d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Token_Classificator (stop_words_list):\n",
    "\n",
    "    vec = CountVectorizer(ngram_range=(1, 1), tokenizer=word_tokenize, stop_words=stop_words_list)\n",
    "    bow = vec.fit_transform(x_train)\n",
    "    clf = LogisticRegression(random_state=42)\n",
    "    clf.fit(bow, y_train)\n",
    "    pred = clf.predict(vec.transform(x_test))\n",
    "\n",
    "    print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f969d21b",
   "metadata": {},
   "source": [
    "*Высокочастотные токены.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3c74b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.60      0.67     35926\n",
      "    positive       0.50      0.69      0.58     20783\n",
      "\n",
      "    accuracy                           0.63     56709\n",
      "   macro avg       0.63      0.64      0.63     56709\n",
      "weighted avg       0.67      0.63      0.64     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Token_Classificator(noise + list(low_tokens) + list(medium_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77c1289",
   "metadata": {},
   "source": [
    "*Среднечастотные токены*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ab7c637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.51      0.67      0.58     21291\n",
      "    positive       0.76      0.62      0.68     35418\n",
      "\n",
      "    accuracy                           0.64     56709\n",
      "   macro avg       0.64      0.64      0.63     56709\n",
      "weighted avg       0.67      0.64      0.64     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Token_Classificator(noise + list(high_tokens) + list(low_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1725f347",
   "metadata": {},
   "source": [
    "*Низкочастотные токены*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2628451a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.73      0.76     30544\n",
      "    positive       0.71      0.78      0.74     26165\n",
      "\n",
      "    accuracy                           0.75     56709\n",
      "   macro avg       0.75      0.75      0.75     56709\n",
      "weighted avg       0.75      0.75      0.75     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Token_Classificator(noise + list(high_tokens) + list(medium_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ab5bd8",
   "metadata": {},
   "source": [
    "#### ВЫВОД: наилучшая точность предсказания у модели с низкочастотными токенами. Однако, стоит заметить, что на точность так же оказывает влияние и количество токенов (количество низкочастотных самое высокое). Если смещать границы и увеличивать количество токенов в группе - то точность тоже начнет увеличиваться. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bed87a",
   "metadata": {},
   "source": [
    "### Задание 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c131d29",
   "metadata": {},
   "source": [
    "Попробуем поработать с пунктуацией. Наше предположение построим на том, что в некоторых случаях пунктуация помогает в классификациях текстов. Посмотрим - просматривается ли какая зависимость в нашем случае."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61417a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00     27901\n",
      "    positive       1.00      1.00      1.00     28808\n",
      "\n",
      "    accuracy                           1.00     56709\n",
      "   macro avg       1.00      1.00      1.00     56709\n",
      "weighted avg       1.00      1.00      1.00     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec = TfidfVectorizer(ngram_range=(1, 1), tokenizer=word_tokenize, stop_words=stopwords.words('russian'))\n",
    "bow = vec.fit_transform(x_train)\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(vec.transform(x_test))\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b28af2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cec017fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.57      0.56      0.57     28370\n",
      "    positive       0.57      0.58      0.57     28339\n",
      "\n",
      "    accuracy                           0.57     56709\n",
      "   macro avg       0.57      0.57      0.57     56709\n",
      "weighted avg       0.57      0.57      0.57     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cool_token = '@'\n",
    "pred = ['positive' if cool_token in tweet else 'negative' for tweet in x_test]\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11c731f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.99      0.49      0.66     56308\n",
      "    positive       0.01      0.50      0.01       401\n",
      "\n",
      "    accuracy                           0.49     56709\n",
      "   macro avg       0.50      0.50      0.34     56709\n",
      "weighted avg       0.99      0.49      0.65     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cool_token = '&'\n",
    "pred = ['positive' if cool_token in tweet else 'negative' for tweet in x_test]\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2db947b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92     32812\n",
      "    positive       0.83      1.00      0.91     23897\n",
      "\n",
      "    accuracy                           0.91     56709\n",
      "   macro avg       0.92      0.93      0.91     56709\n",
      "weighted avg       0.93      0.91      0.91     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cool_token = ')'\n",
    "pred = ['positive' if cool_token in tweet else 'negative' for tweet in x_test]\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2373e6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('!', 0.5173781939374702), ('\"', 0.5057750974272162), ('#', 0.5045230915727662), ('$', 0.49272602232449875), ('%', 0.4948244546720979), ('&', 0.4927965578655945), (\"'\", 0.4927436562097727), ('(', 0.02521645594173764), (')', 0.9141406126011744), ('*', 0.5122643672080269), ('+', 0.4933079405385389), (',', 0.5026362658484544), ('-', 0.5099895960076883), ('.', 0.5056692941155725), ('/', 0.54622723024564), (':', 0.5491368213158405), (';', 0.4968347175933273), ('<', 0.4927436562097727), ('=', 0.49196776525771924), ('>', 0.4927436562097727), ('?', 0.5035355939974254), ('@', 0.5699624398243665), ('[', 0.49263785289812906), ('\\\\', 0.49276129009504666), (']', 0.4927965578655945), ('^', 0.49955033592551445), ('_', 0.5193002874323299), ('`', 0.49233807684847203), ('{', 0.49272602232449875), ('|', 0.4887936659084096), ('}', 0.49272602232449875), ('~', 0.4926907545539509)]\n"
     ]
    }
   ],
   "source": [
    "tokens_arr = []\n",
    "for i in punctuation:\n",
    "    cool_token = i\n",
    "    pred = ['positive' if cool_token in tweet else 'negative' for tweet in x_test]\n",
    "    tokens_arr.append((i,accuracy_score(pred, y_test)))\n",
    "print(tokens_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cabd42",
   "metadata": {},
   "source": [
    "**В общем-то получили достаточно предсказуемый результат: само собой разумеется, что символ \")\" чаще всего будет встречаться в позитивных твитах. А слеовательно, знаки пунктуации в твитах могут быть полезными.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb55c109",
   "metadata": {},
   "source": [
    "### Задание 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51015b1a",
   "metadata": {},
   "source": [
    "Построим модели, используя другие векторайзеры и сравним результаты."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18578527",
   "metadata": {},
   "source": [
    "**CountVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1893b322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.77      0.78     29244\n",
      "    positive       0.76      0.80      0.78     27465\n",
      "\n",
      "    accuracy                           0.78     56709\n",
      "   macro avg       0.78      0.78      0.78     56709\n",
      "weighted avg       0.78      0.78      0.78     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_vec = CountVectorizer(ngram_range=(1, 1), tokenizer=word_tokenize, stop_words=noise)\n",
    "bow = count_vec.fit_transform(x_train)\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(count_vec.transform(x_test))\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbefe3f3",
   "metadata": {},
   "source": [
    "**TfidfVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "29261297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.78      0.77     27771\n",
      "    positive       0.78      0.78      0.78     28938\n",
      "\n",
      "    accuracy                           0.78     56709\n",
      "   macro avg       0.78      0.78      0.78     56709\n",
      "weighted avg       0.78      0.78      0.78     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_vect = TfidfVectorizer(ngram_range=(1, 1), tokenizer=word_tokenize, stop_words=noise)\n",
    "bow = tf_vect.fit_transform(x_train)\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(tf_vect.transform(x_test))\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324e6340",
   "metadata": {},
   "source": [
    "**HashingVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "527e320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479b9db9",
   "metadata": {},
   "source": [
    "Одновременно попробуем подобрать наилучший размер HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4efb638c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_features = 16\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.54      0.52     26047\n",
      "    positive       0.58      0.55      0.56     30662\n",
      "\n",
      "    accuracy                           0.54     56709\n",
      "   macro avg       0.54      0.54      0.54     56709\n",
      "weighted avg       0.55      0.54      0.54     56709\n",
      "\n",
      "========================================================\n",
      "n_features = 64\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.55      0.57      0.56     26806\n",
      "    positive       0.60      0.58      0.59     29903\n",
      "\n",
      "    accuracy                           0.58     56709\n",
      "   macro avg       0.58      0.58      0.58     56709\n",
      "weighted avg       0.58      0.58      0.58     56709\n",
      "\n",
      "========================================================\n",
      "n_features = 256\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.60      0.61      0.60     27294\n",
      "    positive       0.63      0.62      0.62     29415\n",
      "\n",
      "    accuracy                           0.61     56709\n",
      "   macro avg       0.61      0.61      0.61     56709\n",
      "weighted avg       0.61      0.61      0.61     56709\n",
      "\n",
      "========================================================\n",
      "n_features = 1024\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.64      0.65      0.65     27374\n",
      "    positive       0.67      0.66      0.66     29335\n",
      "\n",
      "    accuracy                           0.66     56709\n",
      "   macro avg       0.65      0.66      0.65     56709\n",
      "weighted avg       0.66      0.66      0.66     56709\n",
      "\n",
      "========================================================\n",
      "n_features = 4096\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.70      0.68     27108\n",
      "    positive       0.71      0.69      0.70     29601\n",
      "\n",
      "    accuracy                           0.69     56709\n",
      "   macro avg       0.69      0.69      0.69     56709\n",
      "weighted avg       0.69      0.69      0.69     56709\n",
      "\n",
      "========================================================\n",
      "n_features = 16384\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.70      0.73      0.72     26841\n",
      "    positive       0.75      0.72      0.73     29868\n",
      "\n",
      "    accuracy                           0.73     56709\n",
      "   macro avg       0.73      0.73      0.73     56709\n",
      "weighted avg       0.73      0.73      0.73     56709\n",
      "\n",
      "========================================================\n",
      "n_features = 65536\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.75      0.74     26650\n",
      "    positive       0.77      0.74      0.76     30059\n",
      "\n",
      "    accuracy                           0.75     56709\n",
      "   macro avg       0.75      0.75      0.75     56709\n",
      "weighted avg       0.75      0.75      0.75     56709\n",
      "\n",
      "========================================================\n",
      "n_features = 262144\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.76      0.74     26563\n",
      "    positive       0.78      0.74      0.76     30146\n",
      "\n",
      "    accuracy                           0.75     56709\n",
      "   macro avg       0.75      0.75      0.75     56709\n",
      "weighted avg       0.75      0.75      0.75     56709\n",
      "\n",
      "========================================================\n",
      "n_features = 1048576\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.76      0.74     26608\n",
      "    positive       0.78      0.75      0.76     30101\n",
      "\n",
      "    accuracy                           0.75     56709\n",
      "   macro avg       0.75      0.75      0.75     56709\n",
      "weighted avg       0.75      0.75      0.75     56709\n",
      "\n",
      "========================================================\n",
      "n_features = 4194304\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.76      0.75     26587\n",
      "    positive       0.78      0.75      0.76     30122\n",
      "\n",
      "    accuracy                           0.76     56709\n",
      "   macro avg       0.76      0.76      0.76     56709\n",
      "weighted avg       0.76      0.76      0.76     56709\n",
      "\n",
      "========================================================\n"
     ]
    }
   ],
   "source": [
    "for n in range (4, 24, 2):\n",
    "    vectorizer = HashingVectorizer(n_features=2**n,)\n",
    "    bow = vectorizer.fit_transform(x_train)\n",
    "    clf = LogisticRegression(random_state=42)\n",
    "    clf.fit(bow, y_train)\n",
    "    pred = clf.predict(vectorizer.transform(x_test))\n",
    "    print(f'n_features = {2**n}')\n",
    "    print(classification_report(pred, y_test))\n",
    "    print(56*'=')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e817b6ea",
   "metadata": {},
   "source": [
    "Оптимальное количество фичей в модели с HashingVectorizer - 262144. Так как при дальнейшем увеличении размера векторайзера показатели модели существенно не улучшаются, но сильно возрастает время вычисления."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3f4219",
   "metadata": {},
   "source": [
    "**Напишем и обучим нейронную сеть**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ffad1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (2.10.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow) (0.27.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: setuptools in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow) (63.4.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow) (2.0.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: packaging in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow) (2.10.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow) (4.3.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow) (1.23.1)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow) (3.19.6)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow) (1.49.1)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow) (1.2.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow) (22.9.24)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow) (14.0.6)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.12.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from packaging->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow) (5.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow) (3.8.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (3.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56c0abb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-17 19:36:50.050098: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, Conv1D, GRU, LSTM, Dropout\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc521c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_bin = np.where(y_train=='positive', 1, 0)\n",
    "y_test_bin = np.where(y_test=='positive', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4cebfec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-17 19:37:00.051551: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train_bin))\n",
    "valid_data = tf.data.Dataset.from_tensor_slices((x_test, y_test_bin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb5b84a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.batch(256)\n",
    "valid_data = valid_data.batch(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "daed73ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "valid_data = valid_data.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1f04686b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    \n",
    "    return input_data\n",
    "\n",
    "vocab_size = 15000\n",
    "seq_len = 140\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=seq_len)\n",
    "\n",
    "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "text_data = train_data.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "42ec57eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=250\n",
    "\n",
    "model = Sequential([\n",
    "    vectorize_layer,\n",
    "    Embedding(vocab_size, embedding_dim),\n",
    "    Conv1D(embedding_dim, 3),\n",
    "    Conv1D(embedding_dim, 2),\n",
    "    GRU(350),\n",
    "    Dense(200, activation='relu'),\n",
    "    Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "174e68ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-3)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fccac24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "665/665 [==============================] - 2857s 4s/step - loss: 0.4968 - accuracy: 0.6915 - val_loss: 0.4128 - val_accuracy: 0.7241\n",
      "Epoch 2/2\n",
      "665/665 [==============================] - 3509s 5s/step - loss: 0.4434 - accuracy: 0.7387 - val_loss: 0.5387 - val_accuracy: 0.7198\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb55494a850>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, validation_data=valid_data, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4bc69c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222/222 [==============================] - 286s 1s/step\n"
     ]
    }
   ],
   "source": [
    "logits = model.predict(valid_data)\n",
    "prediction = tf.round(tf.nn.sigmoid(logits))\n",
    "class_preds = np.where(prediction==1, 'positive', 'negative')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6190b27a",
   "metadata": {},
   "source": [
    "Построим матрицу ошибок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "869cd5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.48      0.85      0.62     15943\n",
      "    positive       0.92      0.65      0.76     40766\n",
      "\n",
      "    accuracy                           0.70     56709\n",
      "   macro avg       0.70      0.75      0.69     56709\n",
      "weighted avg       0.79      0.70      0.72     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(class_preds, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981e9b95",
   "metadata": {},
   "source": [
    "**Сеть в этой конфигурациии на этом датасете показала не самый лучший результат и не смогла обогнать классические модели.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bedce2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
